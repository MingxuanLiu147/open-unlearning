# å¦‚ä½•è¿è¡Œ SimNPO on TOFU (Llama-3.2-1B, forget=10%)

## ğŸ¯ ç›®æ ‡

ä½¿ç”¨ **SimNPO ç®—æ³•**åœ¨ **TOFU æ•°æ®é›†**ä¸Šå®Œæˆ unlearning ä»»åŠ¡ï¼š
- æ¨¡å‹ï¼š**Llama-3.2-1B-Instruct**
- Forget æ¯”ä¾‹ï¼š**10%**ï¼ˆçº¦ 176 æ¡æ ·æœ¬ï¼‰
- Retain æ¯”ä¾‹ï¼š**90%**ï¼ˆçº¦ 1584 æ¡æ ·æœ¬ï¼‰
- å¯¹é½è®ºæ–‡ pipeline

---

## ğŸ“ æ–°å¢æ–‡ä»¶æ¸…å•

æˆ‘ä¸ºæ‚¨åˆ›å»ºäº†ä»¥ä¸‹æ–‡ä»¶ï¼š

### 1. **è¿è¡Œè„šæœ¬**
- âœ… `run_simnpo_tofu_1b.sh` - ä¸€é”®è¿è¡Œè„šæœ¬ï¼ˆåŒ…å«è®­ç»ƒ+è¯„ä¼°ï¼‰

### 2. **æ–‡æ¡£**
- âœ… `SIMNPO_TOFU_è¿è¡ŒæŒ‡å—.md` - è¯¦ç»†è¿è¡ŒæŒ‡å—ï¼ˆæ¨èé˜…è¯»ï¼‰
- âœ… `QUICK_START_SimNPO.md` - å¿«é€Ÿå¯åŠ¨å‚è€ƒå¡
- âœ… `README_è¿è¡ŒSimNPO.md` - æœ¬æ–‡æ¡£

### 3. **å·²æœ‰çš„æ ¸å¿ƒæ³¨é‡Šæ–‡æ¡£**
- âœ… `UNLEARNING_GUIDE_CN.md` - å®Œæ•´çš„ unlearning åŸç†å’Œæµç¨‹
- âœ… `ä»£ç æ³¨é‡Šæ€»ç»“.md` - ä»£ç æ³¨é‡Šæ€»ç»“

---

## ğŸš€ ä¸‰ç§è¿è¡Œæ–¹å¼

### æ–¹å¼ 1: ä¸€é”®è¿è¡Œï¼ˆæœ€ç®€å•ï¼‰

```bash
cd /home/liumingxuan/open-unlearning
bash run_simnpo_tofu_1b.sh
```

**ä¼˜ç‚¹ï¼š**
- âœ… è‡ªåŠ¨å®Œæˆè®­ç»ƒ + è¯„ä¼°
- âœ… å‚æ•°å·²ä¼˜åŒ–ï¼ˆå¯¹é½è®ºæ–‡ï¼‰
- âœ… åŒ…å«è¯¦ç»†çš„æ—¥å¿—è¾“å‡º

**é€‚åˆï¼š** å¿«é€Ÿå¼€å§‹ï¼Œä¸éœ€è¦ä¿®æ”¹å‚æ•°

---

### æ–¹å¼ 2: å‘½ä»¤è¡Œè¿è¡Œï¼ˆçµæ´»ï¼‰

#### æ­¥éª¤ 1: è®­ç»ƒæ¨¡å‹

```bash
CUDA_VISIBLE_DEVICES=4 HYDRA_FULL_ERROR=1 python src/train.py \
    --config-name=unlearn.yaml \
    experiment=unlearn/tofu/default \
    trainer=SimNPO \
    forget_split=forget10 \
    retain_split=retain90 \
    task_name=my_simnpo_experiment \
    model.model_args.attn_implementation=eager \
    trainer.args.learning_rate=5e-5 \
    trainer.args.num_train_epochs=20 \
    trainer.args.per_device_train_batch_size=2 \
    trainer.args.gradient_accumulation_steps=8 \
    trainer.args.eval_strategy=steps \
    +trainer.args.eval_steps=100 \
    trainer.args.save_strategy=steps \
    +trainer.args.save_steps=100 \
    trainer.args.logging_steps=10
```

#### æ­¥éª¤ 2: è¯„ä¼°æ•ˆæœ

```bash
CUDA_VISIBLE_DEVICES=4 python src/eval.py \
    --config-name=eval.yaml \
    experiment=eval/tofu/default \
    forget_split=forget10 \
    holdout_split=holdout10 \
    model=Llama-3.2-1B-Instruct \
    task_name=my_simnpo_experiment \
    model.model_args.pretrained_model_name_or_path=saves/unlearn/my_simnpo_experiment \
    paths.output_dir=saves/unlearn/my_simnpo_experiment/evals
```

**ä¼˜ç‚¹ï¼š**
- âœ… å¯ä»¥é€æ­¥æ‰§è¡Œ
- âœ… æ˜“äºä¿®æ”¹å•ä¸ªå‚æ•°
- âœ… æ›´å¥½çš„é”™è¯¯è°ƒè¯•

**é€‚åˆï¼š** éœ€è¦è°ƒæ•´å‚æ•°æˆ–åˆ†æ­¥æ‰§è¡Œ

---

### æ–¹å¼ 3: å¤š GPU å¹¶è¡Œï¼ˆæœ€å¿«ï¼‰

```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch \
    --config_file configs/accelerate/default_config.yaml \
    src/train.py \
    --config-name=unlearn.yaml \
    experiment=unlearn/tofu/default \
    trainer=SimNPO \
    forget_split=forget10 \
    retain_split=retain90 \
    task_name=my_simnpo_experiment \
    trainer.args.learning_rate=5e-5 \
    trainer.args.num_train_epochs=20 \
    trainer.args.per_device_train_batch_size=2 \
    trainer.args.gradient_accumulation_steps=4
```

**ä¼˜ç‚¹ï¼š**
- âœ… è®­ç»ƒé€Ÿåº¦å¿« 2-4 å€
- âœ… æ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶èµ„æº

**é€‚åˆï¼š** æœ‰å¤šå— GPU ä¸”éœ€è¦å¿«é€Ÿå®Œæˆè®­ç»ƒ

---

## âš™ï¸ å…³é”®å‚æ•°ï¼ˆå¯¹é½è®ºæ–‡ï¼‰

### SimNPO ç®—æ³•å‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ | æ¥æº |
|------|-----|------|------|
| `gamma` | 0.125 | Forget æŸå¤±æƒé‡ | [è®ºæ–‡](https://github.com/OPTML-Group/Unlearn-Simple/blob/main/TOFU/config/forget.yaml) |
| `alpha` | 1.0 | Retain æŸå¤±æƒé‡ | è®ºæ–‡ |
| `beta` | 4.5 | æ¸©åº¦å‚æ•° | è®ºæ–‡ |
| `delta` | 0.0 | NLL åç§»é‡ | è®ºæ–‡ |

### è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `learning_rate` | 5e-5 | å­¦ä¹ ç‡ |
| `num_train_epochs` | 20 | è®­ç»ƒè½®æ•° |
| `per_device_train_batch_size` | 2 | æ¯å¡ batch size |
| `gradient_accumulation_steps` | 8 | æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆ BS=16ï¼‰ |
| `warmup_epochs` | 1.0 | Warmup è½®æ•° |

---

## ğŸ“Š é¢„æœŸç»“æœ

### è®­ç»ƒè¾“å‡º

```bash
# è®­ç»ƒæ—¥å¿—ä½ç½®
saves/unlearn/my_simnpo_experiment/trainer_log.txt

# æ¨¡å‹æ£€æŸ¥ç‚¹
saves/unlearn/my_simnpo_experiment/checkpoint-100/
saves/unlearn/my_simnpo_experiment/checkpoint-200/
...

# æœ€ç»ˆæ¨¡å‹
saves/unlearn/my_simnpo_experiment/pytorch_model.bin
```

### è¯„ä¼°æŒ‡æ ‡ï¼ˆå¯¹é½è®ºæ–‡åŸºå‡†ï¼‰

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| **Forget Quality (FQ)** | < 0.10 | æ¨¡å‹åœ¨ forget æ•°æ®ä¸Šçš„æ€§èƒ½åº”è¯¥ä¸‹é™ |
| **Model Utility (MU)** | > 0.85 | æ¨¡å‹åœ¨ retain æ•°æ®ä¸Šçš„æ€§èƒ½åº”è¯¥ä¿æŒ |
| **ç»¼åˆå¾—åˆ†** | ~0.80 | (1 - FQ) Ã— MU |

**è®ºæ–‡ä¸­ SimNPO çš„è¡¨ç°ï¼ˆforget10ï¼‰ï¼š**
- FQ: **0.08**
- MU: **0.87**
- Score: **0.80**

### è¯„ä¼°è¾“å‡º

```bash
# è¯„ä¼°ç»“æœ
saves/unlearn/my_simnpo_experiment/evals/TOFU_EVAL.json

# æŸ¥çœ‹ç»“æœ
cat saves/unlearn/my_simnpo_experiment/evals/TOFU_EVAL.json | jq
```

---

## ğŸ”§ å¸¸è§è°ƒæ•´

### 1. æµ‹è¯•ä¸åŒçš„ forget æ¯”ä¾‹

```bash
# 1% forgetï¼ˆæ›´å®¹æ˜“é—å¿˜ï¼‰
forget_split=forget01 retain_split=retain99

# 5% forget
forget_split=forget05 retain_split=retain95

# 10% forgetï¼ˆé»˜è®¤ï¼‰
forget_split=forget10 retain_split=retain90
```

### 2. è°ƒæ•´é—å¿˜å¼ºåº¦

```bash
# æ›´å¼ºçš„é—å¿˜ï¼ˆä½†å¯èƒ½å½±å“ retain æ€§èƒ½ï¼‰
trainer.method_args.gamma=0.5        # é»˜è®¤ 0.125
trainer.method_args.beta=10.0        # é»˜è®¤ 4.5

# æ›´å¥½çš„æ€§èƒ½ä¿æŒï¼ˆä½†é—å¿˜å¯èƒ½ä¸å½»åº•ï¼‰
trainer.method_args.alpha=2.0        # é»˜è®¤ 1.0
trainer.method_args.gamma=0.05       # é»˜è®¤ 0.125
```

### 3. è°ƒæ•´è®­ç»ƒé€Ÿåº¦

```bash
# æ›´å¿«æ”¶æ•›ï¼ˆä½†å¯èƒ½ä¸ç¨³å®šï¼‰
trainer.args.learning_rate=1e-4      # é»˜è®¤ 5e-5
trainer.args.num_train_epochs=10     # é»˜è®¤ 20

# æ›´ç¨³å®šè®­ç»ƒï¼ˆä½†æ”¶æ•›æ…¢ï¼‰
trainer.args.learning_rate=1e-5      # é»˜è®¤ 5e-5
trainer.args.num_train_epochs=30     # é»˜è®¤ 20
```

### 4. æ˜¾å­˜ä¼˜åŒ–

```bash
# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼ˆ< 12GBï¼‰
trainer.args.per_device_train_batch_size=1      # é»˜è®¤ 2
trainer.args.gradient_accumulation_steps=16     # é»˜è®¤ 8
trainer.args.gradient_checkpointing=true        # é»˜è®¤ false
```

---

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### å®æ—¶æŸ¥çœ‹æ—¥å¿—

```bash
# è®­ç»ƒæ—¥å¿—
tail -f saves/unlearn/my_simnpo_experiment/trainer_log.txt

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### å…³é”®æŒ‡æ ‡

**è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿï¼š**
1. **Loss ä¸‹é™è¶‹åŠ¿**ï¼šåº”è¯¥å¹³ç¨³ä¸‹é™
2. **Eval Loss**ï¼šæ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡
3. **GPU æ˜¾å­˜**ï¼šåº”è¯¥ç¨³å®šåœ¨ 8-12 GBï¼ˆ1B æ¨¡å‹ï¼‰

**è®­ç»ƒå®Œæˆåæ£€æŸ¥ï¼š**
1. **Forget Quality**ï¼šåº”è¯¥ < 0.10
2. **Model Utility**ï¼šåº”è¯¥ > 0.85
3. **ç»¼åˆå¾—åˆ†**ï¼šåº”è¯¥ > 0.80

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ–¹æ¡ˆ 1: å‡å° batch size
trainer.args.per_device_train_batch_size=1
trainer.args.gradient_accumulation_steps=16

# æ–¹æ¡ˆ 2: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
trainer.args.gradient_checkpointing=true

# æ–¹æ¡ˆ 3: ä½¿ç”¨ DeepSpeed
accelerate launch \
    --config_file configs/accelerate/zero_stage3_offload_config.json \
    src/train.py ...
```

### Q2: æ¨¡å‹è·¯å¾„é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
OSError: open-unlearning/tofu_Llama-3.2-1B-Instruct_full not found
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
huggingface-cli repo info open-unlearning/tofu_Llama-3.2-1B-Instruct_full

# æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„
model.model_args.pretrained_model_name_or_path=/path/to/your/model
```

### Q3: è®­ç»ƒæŸå¤±ä¸ä¸‹é™

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**
```bash
# 1. å­¦ä¹ ç‡å¤ªå°
trainer.args.learning_rate=1e-4

# 2. æƒé‡è®¾ç½®ä¸å½“
trainer.method_args.gamma=0.25

# 3. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
trainer.args.logging_steps=1  # å¢åŠ æ—¥å¿—é¢‘ç‡
```

### Q4: è¯„ä¼°å¤±è´¥

**é”™è¯¯ä¿¡æ¯ï¼š**
```
KeyError: 'retain_logs_path'
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æä¾›å‚è€ƒæ¨¡å‹çš„è¯„ä¼°æ—¥å¿—
retain_logs_path=saves/eval/tofu_Llama-3.2-1B-Instruct_retain90/TOFU_EVAL.json

# å¦‚æœä¸å­˜åœ¨ï¼Œè®¾ç½®ä¸º null
retain_logs_path=null
```

---

## âœ… è¿è¡Œå‰æ£€æŸ¥æ¸…å•

- [ ] **ç¯å¢ƒå‡†å¤‡**
  - [ ] GPU å¯ç”¨ï¼ˆè‡³å°‘ 12GB æ˜¾å­˜ï¼‰
  - [ ] Python ç¯å¢ƒå·²å®‰è£…ä¾èµ–
  - [ ] CUDA ç‰ˆæœ¬å…¼å®¹

- [ ] **æ•°æ®å‡†å¤‡**
  - [ ] å¯è®¿é—® HuggingFaceï¼ˆæˆ–å·²ä¸‹è½½æ•°æ®é›†ï¼‰
  - [ ] é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨

- [ ] **é…ç½®ç¡®è®¤**
  - [ ] GPU ç¼–å·æ­£ç¡®ï¼ˆ`CUDA_VISIBLE_DEVICES`ï¼‰
  - [ ] ä»»åŠ¡åç§°å·²è®¾ç½®ï¼ˆ`task_name`ï¼‰
  - [ ] è¾“å‡ºç›®å½•æœ‰å†™å…¥æƒé™

---

## ğŸ“š æ›´å¤šèµ„æº

### æ–‡æ¡£

1. **`QUICK_START_SimNPO.md`** - å¿«é€Ÿå‚è€ƒå¡ï¼ˆæ¨èï¼‰
2. **`SIMNPO_TOFU_è¿è¡ŒæŒ‡å—.md`** - è¯¦ç»†è¿è¡ŒæŒ‡å—
3. **`UNLEARNING_GUIDE_CN.md`** - Unlearning åŸç†è¯¦è§£
4. **`ä»£ç æ³¨é‡Šæ€»ç»“.md`** - ä»£ç æ³¨é‡Šè¯´æ˜

### ä»£ç 

- `src/train.py` - è®­ç»ƒå…¥å£ï¼ˆå·²æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼‰
- `src/trainer/unlearn/simnpo.py` - SimNPO ç®—æ³•å®ç°ï¼ˆå·²æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼‰
- `src/trainer/unlearn/grad_diff.py` - GradDiff åŸºç±»ï¼ˆå·²æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼‰
- `src/data/unlearn.py` - ForgetRetainDatasetï¼ˆå·²æ·»åŠ è¯¦ç»†æ³¨é‡Šï¼‰

### å¤–éƒ¨èµ„æº

- **SimNPO è®ºæ–‡ï¼š** https://github.com/OPTML-Group/Unlearn-Simple
- **TOFU æ•°æ®é›†ï¼š** https://huggingface.co/datasets/locuslab/TOFU
- **LLaMA æ¨¡å‹ï¼š** https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct

---

## ğŸ“ ä¸‹ä¸€æ­¥

### å®ŒæˆåŸºç¡€å®éªŒåï¼š

1. **å¯¹æ¯”å…¶ä»–æ–¹æ³•ï¼š**
   ```bash
   # è¿è¡Œ GradAscentã€GradDiffã€NPO ç­‰è¿›è¡Œå¯¹æ¯”
   trainer=GradAscent / GradDiff / NPO / DPO
   ```

2. **æµ‹è¯•ä¸åŒé…ç½®ï¼š**
   ```bash
   # ä¸åŒçš„ forget æ¯”ä¾‹
   forget_split=forget01 / forget05 / forget10
   
   # ä¸åŒçš„è¶…å‚æ•°
   trainer.method_args.gamma=0.05 / 0.125 / 0.5
   ```

3. **æ·±å…¥åˆ†æï¼š**
   - æŸ¥çœ‹æ¨¡å‹åœ¨ä¸åŒæ ·æœ¬ä¸Šçš„è¡¨ç°
   - åˆ†æé—å¿˜çš„é€‰æ‹©æ€§ï¼ˆæ˜¯å¦åªé—å¿˜äº†ç›®æ ‡æ•°æ®ï¼‰
   - è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿå¼€å§‹è¿è¡Œï¼š**

```bash
bash run_simnpo_tofu_1b.sh
```

**é¢„è®¡æ—¶é—´ï¼š** 2-3 å°æ—¶ï¼ˆå•å¡ A100/V100ï¼‰

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸ‰
